{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled14.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOv03juzfh8oXYXmG/4DPFV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nagulapalliabhinay/CLV_PREDICTION/blob/main/Untitled14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA8NhQXyaJXf"
      },
      "outputs": [],
      "source": [
        "# Importing the necessary libraries\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "from zipfile import ZipFile\n",
        "# !pip install plotly\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime as dt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import plotnine as pn\n",
        "import plydata.cat_tools as cat\n",
        "# machine learning\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# Importing the lifetimes package\n",
        "import lifetimes\n",
        "from lifetimes.plotting import plot_probability_alive_matrix\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Loading the Data\n",
        "\n",
        "# Laoding the data into a dataframe.\n",
        "CDNOW_DATA = pd.read_csv(\"Data/CDNOW_master.txt\", sep='\\s+')\n",
        "CDNOW_DATA.shape\n",
        "\n",
        "CDNOW_DATA.head() # Looking at the top 5 rows of the dataframe.\n",
        "\n",
        "# Data Cleaning\n",
        "\n",
        "CDNOW_DATA.info() # info() gives us the number of non null values and the data type of each column.\n",
        "\n",
        "CDNOW_DATA.columns = [\"CustomerID\", \"TransactionDate\", \"UnitsSold\", \"Price\"] # Assigning the column names to the data frame.\n",
        "\n",
        "CDNOW_DATA[\"CustomerID\"] = CDNOW_DATA[\"CustomerID\"].astype(str) # changing the datatype of the CustomerID to string.\n",
        "\n",
        "# changing the datatye of the TransactionDate column to datetime.\n",
        "CDNOW_DATA['TransactionDate'] = CDNOW_DATA['TransactionDate'].apply(lambda x: pd.to_datetime(str(x), format='%Y-%m-%d'))\n",
        "\n",
        "CDNOW_DATA.head()  # Looking at the top 5 rows of the dataframe.\n",
        "\n",
        "# Descriptive Analytics\n",
        "\n",
        "CDNOW_DATA.describe() # looking at the min, max, mean, standard deviation and quartile values of the numerical values.\n",
        "\n",
        "# First Transaction Date\n",
        "CDNOW_DATA[\"TransactionDate\"].min()\n",
        "\n",
        "# Last Transaction Date\n",
        "CDNOW_DATA[\"TransactionDate\"].max()\n",
        "\n",
        "# NUmber of Unique Customers\n",
        "CDNOW_DATA[\"CustomerID\"].nunique()\n",
        "\n",
        "# Total Revenue.\n",
        "CDNOW_DATA[\"Price\"].sum()\n",
        "\n",
        "### Customers with highest number of Transactions\n",
        "\n",
        "# Grouping the data to create a dataframe with CustomerID and number of transactions that customer made.\n",
        "NO_OF_TRANSACTIONS = CDNOW_DATA.groupby(\"CustomerID\")[\"UnitsSold\"].count() \\\n",
        "                                        .reset_index().sort_values(\"UnitsSold\", ascending = False) \\\n",
        "                                        .rename(columns = {\"UnitsSold\" : \"NO_OF_TRANSACTIONS\"}) \\\n",
        "                                        .reset_index(drop = True) \n",
        "\n",
        "\n",
        "TOP_10_HIGHEST_TRANSACTIONS = NO_OF_TRANSACTIONS.iloc[:10, :2] # slicing to get just the 10 rows worth of data.\n",
        "TOP_10_HIGHEST_TRANSACTIONS[\"CustomerID\"] = TOP_10_HIGHEST_TRANSACTIONS[\"CustomerID\"].astype(\"str\")\n",
        "TOP_10_HIGHEST_TRANSACTIONS.shape\n",
        "\n",
        "# plt.bar() function is used for plotting the bar plot.\n",
        "# plt.ylabel() function is used for labelling the Y-axis.\n",
        "# plt.xlabel() function is used for labelling the X-axis.\n",
        "# plt.title() function is used for setting the title for the plot.\n",
        "# plt.legend() function is used for setting the legend for the plot\n",
        "# plt.show() function is used for displaying the plot.\n",
        "\n",
        "        \n",
        "#declaring the legend for the plot.\n",
        "colors = {'CUSTOMER WITH HIGHEST NUMBER OF TRANSACTIONS':'green'}         \n",
        "labels = list(colors.keys())\n",
        "handles = [plt.Rectangle((0,0),1,1, color=colors[label]) for label in labels]\n",
        "plt.legend(handles, labels, bbox_to_anchor=(1.05, 1))\n",
        "\n",
        "clrs1=[]\n",
        "# declaring the colors of the bar plot's bars.\n",
        "for i in TOP_10_HIGHEST_TRANSACTIONS['NO_OF_TRANSACTIONS']:\n",
        "    if(i==max(TOP_10_HIGHEST_TRANSACTIONS['NO_OF_TRANSACTIONS'])):\n",
        "        clrs1.append('green')\n",
        "    else:\n",
        "        clrs1.append('blue')\n",
        "        \n",
        "\n",
        "#Plot\n",
        "bar_plot=plt.bar(TOP_10_HIGHEST_TRANSACTIONS['CustomerID'], TOP_10_HIGHEST_TRANSACTIONS['NO_OF_TRANSACTIONS'], color=clrs1)\n",
        "plt.title('10 CUSTOMERS WITH HIGHEST TRANSACTIONS', pad =25,fontweight='bold', fontname=\"Times New Roman\", style='italic')\n",
        "plt.ylabel('NUMBER OF TRANSACTIONS', fontweight='bold', color = 'Black', verticalalignment='center', labelpad=30, fontname=\"Times New Roman\", style='italic') # ylabel function is used for labelling the Y-axis.\n",
        "plt.xlabel('CUSTOMER ID',fontweight='bold', color = 'Black', horizontalalignment='center', labelpad=30, fontname=\"Times New Roman\", style='italic') # xlabel function is used for labelling the X-axis.\n",
        "plt.xticks(rotation='vertical')\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n",
        "MONTHLY_INCOME = pd.DataFrame(CDNOW_DATA.set_index('TransactionDate')[\"Price\"].resample(\"M\").sum())\n",
        "MONTHLY_INCOME.head()\n",
        "\n",
        "# Plotting the bar plot of Transaction_data and Price.\n",
        "\n",
        "fig = go.Figure([go.Scatter(x=MONTHLY_INCOME.index, y=MONTHLY_INCOME['Price'])])\n",
        "fig.update_layout(\n",
        "    title=\"MONTHLY INCOME\",\n",
        "    xaxis_title=\"DATE\",\n",
        "    yaxis_title=\"INCOME\",\n",
        "    font=dict(\n",
        "        family=\"Courier New, monospace\",\n",
        "        size=18,\n",
        "        color=\"#7f7f7f\"\n",
        "    ),\n",
        ")\n",
        "fig.show()\n",
        "# Reference:\n",
        "# https://plotly.com/python/time-series/\n",
        "# https://plotly.com/python/reference/layout/\n",
        "\n",
        "### Cohort Analysis\n",
        "\n",
        "# Customers first transaction. Considering the first transaction of the customers as a subset to perfrom our analysis.\n",
        "CDNOW_FIRST_TRANSACTION = CDNOW_DATA.sort_values([\"CustomerID\", \"TransactionDate\"]) \\\n",
        "                               .groupby(\"CustomerID\").first()\n",
        "CDNOW_FIRST_TRANSACTION.shape\n",
        "\n",
        "# first new customer.\n",
        "CDNOW_FIRST_TRANSACTION[\"TransactionDate\"].min()\n",
        "\n",
        "# last new customer\n",
        "CDNOW_FIRST_TRANSACTION[\"TransactionDate\"].max()\n",
        "\n",
        "# GETTING THE UNIQUE CUSTOMERS ID's\n",
        "IDs = CDNOW_DATA['CustomerID'].unique()\n",
        "IDs_SAMPLE = IDs[0:10]\n",
        "\n",
        "# Creating a subset of the dataset with ten unique customer IDs\n",
        "CDNOW_SUBSET = CDNOW_DATA[CDNOW_DATA['CustomerID'].isin(IDs_SAMPLE)].groupby(['CustomerID', 'TransactionDate']).sum().reset_index()\n",
        "CDNOW_SUBSET.head()\n",
        "\n",
        "# Frequency plot of the ten unique customers, with the price and date of each transaction.\n",
        "pn.ggplot(pn.aes('TransactionDate', 'Price', group = 'CustomerID'), data = CDNOW_SUBSET) + pn.geom_line() +pn.geom_point() +pn.facet_wrap('CustomerID') +pn.scale_x_date(date_breaks = '1 year', date_lables = '%Y')\n",
        "\n",
        "# Customer Segmentation <br> <i> RFM analysis</i>\n",
        "\n",
        "### Recency\n",
        "\n",
        "# MOST RECENT TRANSACTIONS OF THE CUSTOMERS \n",
        "# RFM (\"R\": Recency)\n",
        "\n",
        "CDNOW_Recency = CDNOW_DATA.groupby('CustomerID', as_index=False)['TransactionDate'].max()\n",
        "CDNOW_Recency.columns = ['CustomerID', 'LAST_TransactionDate']\n",
        "RECENT_TransactionDate = CDNOW_Recency['LAST_TransactionDate'].max()\n",
        "CDNOW_Recency['Recency'] = CDNOW_Recency['LAST_TransactionDate'].apply(lambda x: (RECENT_TransactionDate - x).days)\n",
        "CDNOW_Recency.head()\n",
        "\n",
        "CDNOW_Recency[\"Recency\"].describe() # Gives the min, max and mean values of the Recency.\n",
        "\n",
        "### Frequency\n",
        "\n",
        "# CALCULACTING THE Frequency OF THE CUSTOMERS\n",
        "# RFM (\"F\": Frequency)\n",
        "\n",
        "CDNOW_Frequency = CDNOW_DATA.drop_duplicates().groupby('CustomerID', as_index=False)['TransactionDate'].count()\n",
        "CDNOW_Frequency.columns = ['CustomerID', 'Frequency']\n",
        "CDNOW_Frequency.head()\n",
        "\n",
        "CDNOW_Frequency[\"Frequency\"].describe() # Gives the min, max and mean values of the Frequency.\n",
        "\n",
        "### Monetary\n",
        "\n",
        "# CALCULATING THE Monetary VALUE OF CUSTOMER SPEND ON PURCHASING CD's FROM THE CDNOW STORE.\n",
        "# RFM (\"M\": Monetary)\n",
        "\n",
        "CDNOW_Monetary = CDNOW_DATA.groupby('CustomerID', as_index=False)['Price'].sum()\n",
        "CDNOW_Monetary.columns = ['CustomerID', 'Monetary']\n",
        "CDNOW_Monetary.head()\n",
        "\n",
        "CDNOW_Monetary[\"Monetary\"].describe() # Gives the min, max and mean values of the Monetary.\n",
        "\n",
        "### RFM\n",
        "\n",
        "# MERGING Recency Frequency AND Monetary\n",
        "CDNOW_RF = CDNOW_Recency.merge(CDNOW_Frequency, on='CustomerID')\n",
        "CDNOW_RFM = CDNOW_RF.merge(CDNOW_Monetary, on='CustomerID').drop(columns='LAST_TransactionDate')\n",
        "CDNOW_RFM.head()\n",
        "\n",
        "plt.rcParams.update(plt.rcParamsDefault)\n",
        "fig, axes = plt.subplots(1,3, figsize=(18, 5))\n",
        "sns.distplot(CDNOW_RFM[\"Recency\"], ax = axes[0])\n",
        "sns.distplot(CDNOW_RFM[\"Frequency\"], ax = axes[1])\n",
        "sns.distplot(CDNOW_RFM[\"Monetary\"], ax = axes[2])\n",
        "plt.show()\n",
        "\n",
        "# From the plots below we can see that the data has skewness and we need to normalize the data.\n",
        "\n",
        "fig, axes = plt.subplots(1,3, figsize=(18, 5))\n",
        "sns.boxplot(x = CDNOW_RFM[\"Recency\"], ax = axes[0])\n",
        "sns.boxplot(x = CDNOW_RFM[\"Frequency\"], ax = axes[1])\n",
        "sns.boxplot(x = CDNOW_RFM[\"Monetary\"], ax = axes[2])\n",
        "plt.show()\n",
        "# From looking at the plot below we say that the data has some outliers.\n",
        "\n",
        "### Robust Scaler\n",
        "\n",
        "# Normalizing the data using Robust Scalar to deal with the coutliers.\n",
        "scaler = RobustScaler()\n",
        "scaler.fit(CDNOW_RFM[['Recency', 'Frequency', 'Monetary']])\n",
        "CDNOW_RFM[['Recency_NORM', 'Frequency_NORM', 'Monetary_NORM']] = scaler.transform(CDNOW_RFM[['Recency', 'Frequency', 'Monetary']])\n",
        "\n",
        "fig, axes = plt.subplots(1,3, figsize=(18, 5))\n",
        "sns.distplot(CDNOW_RFM[\"Recency_NORM\"], ax = axes[0])\n",
        "sns.distplot(CDNOW_RFM[\"Frequency_NORM\"], ax = axes[1])\n",
        "sns.distplot(CDNOW_RFM[\"Monetary_NORM\"], ax = axes[2])\n",
        "plt.show()\n",
        "\n",
        "### K-means for customer segmentation.\n",
        "\n",
        "# Using the Unsupervised Learning to cluster the customers into segemets\n",
        "\n",
        "cluster_range = range( 1, 10 )\n",
        "cluster_errors = {\"R_cluster_errors\":[], \"F_cluster_errors\":[], \"M_cluster_errors\":[]}\n",
        "\n",
        "# Identifing the optimal number of clusters using telbow curve.\n",
        "for num_clusters in cluster_range:\n",
        "    R_clusters = KMeans(num_clusters)\n",
        "    F_clusters = KMeans(num_clusters)\n",
        "    M_clusters = KMeans(num_clusters)\n",
        "    R_clusters.fit(CDNOW_RFM[['Recency_NORM']])\n",
        "    F_clusters.fit(CDNOW_RFM[['Frequency_NORM']])\n",
        "    M_clusters.fit(CDNOW_RFM[['Monetary_NORM']])\n",
        "    cluster_errors[\"R_cluster_errors\"].append(R_clusters.inertia_)\n",
        "    cluster_errors[\"F_cluster_errors\"].append(F_clusters.inertia_)\n",
        "    cluster_errors[\"M_cluster_errors\"].append(M_clusters.inertia_)\n",
        "    \n",
        "\n",
        "# Creating a dataframe of the Cluster Errors to find the optimal Cluster threshold.\n",
        "CLUSTER_ERRORS_DF = pd.DataFrame({\"NO_OF_CLUSTERS\":cluster_range, \"R_CLUSTER_ERRORS\": cluster_errors[\"R_cluster_errors\"], \n",
        "                                  \"F_CLUSTER_ERRORS\": cluster_errors[\"F_cluster_errors\"], \n",
        "                                 \"M_CLUSTER_ERRORS\": cluster_errors[\"M_cluster_errors\"]})\n",
        "\n",
        "CLUSTER_ERRORS_DF\n",
        "\n",
        "# plotting the line graph to picture the elbow curve and determine the optimal number of clusters.\n",
        "plt.rcParams.update(plt.rcParamsDefault)\n",
        "fig, axes = plt.subplots(1,3, figsize=(18, 5))\n",
        "axes[0].plot( CLUSTER_ERRORS_DF.NO_OF_CLUSTERS, CLUSTER_ERRORS_DF.R_CLUSTER_ERRORS, marker = \"o\", axes = axes[0])\n",
        "axes[0].set_title(\"Recency\")\n",
        "axes[0].set_xlabel(\"Number of clusters\")\n",
        "axes[0].set_ylabel(\"Cluster Error\")\n",
        "axes[1].plot( CLUSTER_ERRORS_DF.NO_OF_CLUSTERS, CLUSTER_ERRORS_DF.F_CLUSTER_ERRORS, marker = \"o\", axes = axes[1])\n",
        "axes[1].set_title(\"Frequency\")\n",
        "axes[1].set_xlabel(\"Number of clusters\")\n",
        "axes[1].set_ylabel(\"Cluster Error\")\n",
        "axes[2].plot( CLUSTER_ERRORS_DF.NO_OF_CLUSTERS, CLUSTER_ERRORS_DF.M_CLUSTER_ERRORS, marker = \"o\", axes = axes[2])\n",
        "axes[2].set_title(\"Monetary\")\n",
        "axes[2].set_xlabel(\"Number of clusters\")\n",
        "axes[2].set_ylabel(\"Cluster Error\")\n",
        "plt.subplots_adjust(left  = 0.05,\n",
        "right = 0.9,\n",
        "bottom = 0.1,\n",
        "top = 0.9,\n",
        "wspace = 0.2,\n",
        "hspace = 1)\n",
        "plt.show()\n",
        "\n",
        "# From the Elbow Curves below:\n",
        "# FOR Recency_NORM optimal clusters = 3\n",
        "# FOR Frequency_NORM optimal clusters = 4\n",
        "# FOR Monetary_NORM optimal clusters = 4\n",
        "\n",
        "### K-means\n",
        "\n",
        "# Creating the clusters.\n",
        "\n",
        "R_clusters_Optimized = KMeans(3, random_state=42)\n",
        "F_clusters_Optimized = KMeans(4, random_state=42)\n",
        "M_clusters_Optimized = KMeans(4, random_state=42)\n",
        "\n",
        "# fitting the data\n",
        "R_clusters_Optimized.fit(CDNOW_RFM[['Recency_NORM']])\n",
        "F_clusters_Optimized.fit(CDNOW_RFM[['Frequency_NORM']])\n",
        "M_clusters_Optimized.fit(CDNOW_RFM[['Monetary_NORM']])\n",
        "\n",
        "# Storing the output/labels of the datapoints.\n",
        "CDNOW_RFM[\"R_CLUSTER\"] = R_clusters_Optimized.labels_\n",
        "CDNOW_RFM[\"F_CLUSTER\"] = F_clusters_Optimized.labels_\n",
        "CDNOW_RFM[\"M_CLUSTER\"] = M_clusters_Optimized.labels_\n",
        "\n",
        "# Count plots to see the count of datapoints in each cluster.\n",
        "sns.countplot(CDNOW_RFM[\"R_CLUSTER\"])\n",
        "plt.show()\n",
        "\n",
        "# Count plots to see the count of datapoints in each cluster.\n",
        "sns.countplot(CDNOW_RFM[\"F_CLUSTER\"])\n",
        "plt.show()\n",
        "\n",
        "# Count plots to see the count of datapoints in each cluster.\n",
        "sns.countplot(CDNOW_RFM[\"M_CLUSTER\"])\n",
        "plt.show()\n",
        "\n",
        "CDNOW_RFM.head() # looking at the cluster values.\n",
        "\n",
        "# For a Customer to be a High-Value Customer, the customers:\n",
        "# Recency should be low\n",
        "# Frequency should be high\n",
        "# Monetary Value should be high\n",
        "CDNOW_RFM.sort_values('Recency_NORM', ascending = True, inplace = True)\n",
        "CDNOW_RFM.sort_values('Frequency_NORM', ascending = False, inplace = True)\n",
        "CDNOW_RFM.sort_values('Monetary_NORM', ascending = False, inplace = True)\n",
        "CDNOW_RFM.head()\n",
        "\n",
        "# Creating the final cluster using the independent clusters.\n",
        "CDNOW_RFM[\"FinalCluster\"] = CDNOW_RFM[\"R_CLUSTER\"] + CDNOW_RFM[\"F_CLUSTER\"] + CDNOW_RFM[\"M_CLUSTER\"]\n",
        "CDNOW_RFM.reset_index(inplace = True, drop = True)\n",
        "CDNOW_RFM.head()\n",
        "\n",
        "CDNOW_RFM.tail()\n",
        "\n",
        "plt.plot(CDNOW_RFM.index, CDNOW_RFM[\"FinalCluster\"])\n",
        "plt.show()\n",
        "\n",
        "# Naming the clusters.\n",
        "\n",
        "CDNOW_RFM['Segment'] = 'Lost Customer'\n",
        "CDNOW_RFM.loc[CDNOW_RFM['FinalCluster']==1,'Segment'] = 'Low-Value' \n",
        "CDNOW_RFM.loc[CDNOW_RFM['FinalCluster'] == 2,'Segment'] = 'Mid-Value'\n",
        "CDNOW_RFM.loc[CDNOW_RFM['FinalCluster']>2,'Segment'] = 'High-Value'\n",
        "CDNOW_RFM.loc[CDNOW_RFM['FinalCluster']>5,'Segment'] = 'Top Customer'\n",
        "\n",
        "sns.countplot(CDNOW_RFM['Segment'], order= ['Lost Customer','Low-Value' ,'Mid-Value','High-Value','Top Customer'])\n",
        "plt.title(\"RFM Segmentation of CDNOW dataset\")\n",
        "plt.show()\n",
        "\n",
        "CDNOW_RFM.to_csv(\"CDNOW_CustomerSegmentation.csv\", index = False)\n",
        "\n",
        "# Machine Learning Modelling\n",
        "\n",
        "### Feature Engineering\n",
        "\n",
        "n_days = 90 # The no of days for which we want to predict CLV.\n",
        "max_date = CDNOW_DATA['TransactionDate'].max() # Getting the max transaction date.\n",
        "cutoff = max_date - pd.to_timedelta(n_days, unit = 'd') # Creating a cut off to exclude the last 90 days.\n",
        "\n",
        "IN_DATA = CDNOW_DATA[CDNOW_DATA['TransactionDate'] <= cutoff] # Excluding the last 90 days.\n",
        "OUT_DATA = CDNOW_DATA[CDNOW_DATA['TransactionDate'] > cutoff] # Just the last 90 days data.\n",
        "\n",
        "# making targets from out data\n",
        "TARGET_DATA = OUT_DATA.drop('UnitsSold', axis = 1).groupby('CustomerID').sum().rename({'Price':'spend_90_total'}, axis = 1).assign(spend_90_flag = 1)\n",
        "TARGET_DATA.head()\n",
        "\n",
        "# make recency (date) features from in data\n",
        "max_date = IN_DATA['TransactionDate'].max()\n",
        "\n",
        "# Getting the recency data to include that as feature for ML modelling\n",
        "recency_features_df = IN_DATA[['CustomerID', 'TransactionDate']].groupby('CustomerID').apply(lambda x: (x['TransactionDate'].max() - max_date)/ pd.to_timedelta(1, 'day')).to_frame().set_axis(['recency'], axis = 1)\n",
        "recency_features_df.head()\n",
        "\n",
        "# make frequency (count features from in data)\n",
        "frequency_features_df = IN_DATA[['CustomerID', 'TransactionDate']].groupby('CustomerID').count().set_axis(['frequency'], axis = 1)\n",
        "frequency_features_df.head()\n",
        "\n",
        "# make price (monitery features from in data)\n",
        "price_features_df = IN_DATA.groupby('CustomerID').aggregate({'Price':['sum', 'mean']}).set_axis(['price_sum', 'price_mean'], axis = 1)\n",
        "price_features_df.head()\n",
        "\n",
        "# combine features\n",
        "features_df = pd.concat([recency_features_df, frequency_features_df, price_features_df], axis = 1).merge(TARGET_DATA, left_index = True, right_index = True, how = 'left').fillna(0)\n",
        "features_df.head()\n",
        "\n",
        "### Regression\n",
        "> How much will the customers spend in the next 90 days?\n",
        "\n",
        "### XGBOOST\n",
        "\n",
        "X = features_df[['recency', 'frequency', 'price_sum', 'price_mean']]\n",
        "\n",
        "# next 90 days spend prediction\n",
        "y_spend = features_df['spend_90_total']\n",
        "\n",
        "# Model\n",
        "xgb_reg_spec = XGBRegressor(objective='reg:squarederror', random_state=123)\n",
        "\n",
        "# Creating a GridSearchCV to tune the hyperparameters\n",
        "xgb_reg_model =GridSearchCV(estimator=xgb_reg_spec, param_grid = dict(learning_rate = [0.01, 0.1, 0.3, 0.5]), scoring='neg_mean_absolute_error', cv = 5) \n",
        "\n",
        "# Fitting the Model\n",
        "xgb_reg_model.fit(X, y_spend)\n",
        "\n",
        "xgb_reg_model.best_score_ # NAE of the best parameters model.\n",
        "\n",
        "xgb_reg_model.best_params_ # best Hyperparameter value\n",
        "\n",
        "predictions_xgb_reg_model = xgb_reg_model.predict(X) # Prediction\n",
        "\n",
        "# Merging the predictions to the data.\n",
        "XGB_PRED_DATA = pd.DataFrame({\"prediction_xgb\" : predictions_xgb_reg_model.tolist()}).merge(X.reset_index(),left_index=True, right_index=True)\n",
        "XGB_PRED_DATA.head()\n",
        "\n",
        "XGB_PRED_DATA.to_csv(\"XGB_reg_prediction.csv\") # Writing the predictions to a csv\n",
        "\n",
        "### Linear Regression\n",
        "\n",
        "lr = LinearRegression() # Model\n",
        "\n",
        "parameters = {'fit_intercept':[True,False],  'copy_X':[True, False]} # Hyper parameters.\n",
        "lr_reg_model = GridSearchCV(lr,parameters, cv=5, scoring='neg_mean_absolute_error')\n",
        "lr_reg_model.fit(X, y_spend)\n",
        "\n",
        "lr_reg_model.best_score_ # NAE of the best parameters model.\n",
        "\n",
        "lr_reg_model.best_params_ # best Hyperparameter value\n",
        "\n",
        "predictions_lr_reg_model = lr_reg_model.predict(X) # Prediction\n",
        "\n",
        "# Merging the prediction to the X data.\n",
        "LINEAR_REGRESSION_PRED_DATA = pd.DataFrame({\"prediction_lr\" : predictions_lr_reg_model.tolist()}).merge(X.reset_index(),left_index=True, right_index=True)\n",
        "\n",
        "LINEAR_REGRESSION_PRED_DATA.to_csv(\"lr_reg_prediction.csv\") # Predictions to CSV\n",
        "\n",
        "### Random Forest Regression\n",
        "\n",
        "# Hyper Parameters\n",
        "param_grid = dict(n_estimators = [10, 20, 50, 100, 500, 1000], max_depth = [2, 5], max_features = ['auto', 'sqrt', 'log2'])\n",
        "\n",
        "rf = RandomForestRegressor() # model\n",
        "rf_regressor =GridSearchCV(estimator=rf, param_grid = param_grid, scoring='neg_mean_absolute_error', refit= True, cv = 5) \n",
        "\n",
        "rf_regressor.fit(X, y_spend) #fitting the data\n",
        "\n",
        "rf_regressor.best_score_ # NAE of the best model.\n",
        "\n",
        "rf_regressor.best_params_ # Best hyper parameters.\n",
        "\n",
        "rf_regressor.best_estimator_\n",
        "\n",
        "predictions_rf_reg_model = rf_regressor.predict(X) # Prediction\n",
        "\n",
        "# Mergeing the prediction with X data\n",
        "RF_REGRESSION_PRED_DATA = pd.DataFrame({\"prediction_rf\" : predictions_rf_reg_model.tolist()}).merge(X.reset_index(),left_index=True, right_index=True)\n",
        "\n",
        "RF_REGRESSION_PRED_DATA.to_csv(\"rf_reg_prediction.csv\") # Storing the Prediction data in a csv.\n",
        "\n",
        "### Classification\n",
        "> Will a customer make a purchase in next 90 days\n",
        "\n",
        "# Next 90 days spend probability\n",
        "y_prob = features_df['spend_90_flag']\n",
        "\n",
        "xgb_clf_spec = XGBClassifier(objective='binary:logistic', random_state= 123) # model\n",
        "\n",
        "xgb_clf_model =GridSearchCV(estimator=xgb_clf_spec, param_grid = dict(learning_rate = [0.01, 0.1, 0.3, 0.5]), scoring='roc_auc', refit= True, cv = 5) \n",
        "\n",
        "xgb_clf_model.fit(X, y_prob) # Fitting the data\n",
        "\n",
        "xgb_clf_model.best_score_ # AUC score of the best model\n",
        "\n",
        "xgb_clf_model.best_params_ # Best Hyperparameter Values\n",
        "\n",
        "# Storing the prediction values in a dataframe\n",
        "XGB_CLASSIFICATION_PRED_DATA = pd.DataFrame(xgb_clf_model.predict_proba(X),columns=['prediction_xgb_clf_0','prediction_xgb_clf_1']).merge(X.reset_index(),left_index=True, right_index=True)\n",
        "XGB_CLASSIFICATION_PRED_DATA.head()\n",
        "\n",
        "XGB_CLASSIFICATION_PRED_DATA.to_csv(\"xgb_clf_prediction.csv\") # Predictions to csv\n",
        "\n",
        "### Logistic Regression\n",
        "\n",
        "parameters = {'penalty': [\"l1\", \"l2\"], 'solver': [\"liblinear\",\"saga\"]} # Hyper parameters\n",
        "\n",
        "logr = LogisticRegression() # model\n",
        "logr_clf_model = GridSearchCV(logr,parameters, cv=5, scoring='accuracy')\n",
        "logr_clf_model.fit(X, y_prob) #fitting\n",
        "\n",
        "logr_clf_model.best_params_ # Best Hyperparameters\n",
        "\n",
        "logr_clf_model.best_score_ # AUC score of the best model\n",
        "\n",
        "# Storing the prediction values in a dataframe with X data\n",
        "LOGISTIC_CLASSIFICATION_PRED_DATA = pd.DataFrame(logr_clf_model.predict_proba(X),columns=['prediction_log_clf_0','prediction_log_clf_1']).merge(X.reset_index(),left_index=True, right_index=True)\n",
        "LOGISTIC_CLASSIFICATION_PRED_DATA.head()\n",
        "\n",
        "LOGISTIC_CLASSIFICATION_PRED_DATA.to_csv(\"log_clf_prediction.csv\") # Prediction to csv.\n",
        "\n",
        "### RANDOM FOREST CLASSIFICATION\n",
        "\n",
        "#Hyper Parameters\n",
        "parameters = { 'max_depth' : [2, 5], 'n_estimators' : [10, 20, 50, 100, 500, 1000],'max_features' : ['auto', 'sqrt', 'log2']}\n",
        "\n",
        "rfc = RandomForestClassifier() # mdoel\n",
        "rfc_clf_model = GridSearchCV(rfc, parameters, cv=5, scoring='roc_auc') \n",
        "rfc_clf_model.fit(X, y_prob) # fitting the data\n",
        "\n",
        "rfc_clf_model.best_params_ # Best parameters\n",
        "\n",
        "rfc_clf_model.best_score_ # AUC score of the best model.\n",
        "\n",
        "# Storing the prediction values\n",
        "RF_CLASSIFICATION_PRED_DATA = pd.DataFrame(rfc_clf_model.predict_proba(X),columns=['prediction_rf_clf_0','prediction_rf_clf_1']).merge(X.reset_index(),left_index=True, right_index=True)\n",
        "RF_CLASSIFICATION_PRED_DATA.head()\n",
        "\n",
        "RF_CLASSIFICATION_PRED_DATA.to_csv(\"rf_clf_prediction.csv\") # Predictions to csv.\n",
        "\n",
        "### Feature importance\n",
        "\n",
        "#importance | spend amount model\n",
        "\n",
        "imp_spend_amount_dict = xgb_reg_model.best_estimator_.get_booster().get_score(importance_type = 'gain')\n",
        "\n",
        "imp_spend_amount_df = pd.DataFrame(data = {'feature': list(imp_spend_amount_dict.keys()), \n",
        "                                   'value': list(imp_spend_amount_dict.values())}).assign(feature = lambda x : cat.cat_reorder(x['feature'], x['value']))\n",
        "\n",
        "imp_spend_amount_df\n",
        "\n",
        "pn.ggplot(pn.aes('feature', 'value'), data = imp_spend_amount_df) + pn.geom_col() + pn.coord_flip() #plot of feature importance.\n",
        "\n",
        "# Importance | Spend probability model\n",
        "imp_spend_prob_dict = xgb_clf_model.best_estimator_.get_booster().get_score(importance_type = 'gain')\n",
        "\n",
        "imp_spend_prob_df = pd.DataFrame(data = {'feature': list(imp_spend_prob_dict.keys()), \n",
        "                                   'value': list(imp_spend_prob_dict.values())}).assign(feature = lambda x : cat.cat_reorder(x['feature'], x['value']))\n",
        "\n",
        "pn.ggplot(pn.aes('feature', 'value'), data = imp_spend_prob_df) + pn.geom_col() + pn.coord_flip() #plot of feature importance.\n",
        "\n",
        "# CLV Mathematically\n",
        "\n",
        "# Creating a customer level dataframe\n",
        "customer_level_data = CDNOW_DATA.groupby(\"CustomerID\").aggregate({\"TransactionDate\": lambda x: (x.max() - x.min()).days,\n",
        "                                                              \"CustomerID\": lambda x: len(x),\n",
        "                                                   \"Price\" : lambda x: x.sum()})\n",
        "customer_level_data.columns = ['num_days', 'num_transactions', 'spent_money']\n",
        "customer_level_data.head()\n",
        "\n",
        "### AVG_ORDER_VALUE\n",
        "\n",
        "#Calculating the average order value for each customer\n",
        "customer_level_data['avg_order_value']=customer_level_data['spent_money']/customer_level_data['num_transactions']\n",
        "customer_level_data.head()\n",
        "\n",
        "### PURCHASE FREQUNECY\n",
        "\n",
        "purchase_frequency=sum(customer_level_data['num_transactions'])/customer_level_data.shape[0]\n",
        "purchase_frequency\n",
        "\n",
        "### REPEAT RATE\n",
        "\n",
        "# Repeat Rate\n",
        "repeat_rate=customer_level_data[customer_level_data.num_transactions > 1].shape[0]/customer_level_data.shape[0]\n",
        "repeat_rate\n",
        "\n",
        "### CHURN\n",
        "\n",
        "#Churn Rate\n",
        "churn_rate=1-repeat_rate\n",
        "churn_rate\n",
        "\n",
        "purchase_frequency,repeat_rate,churn_rate\n",
        "\n",
        "### PROFIT\n",
        "\n",
        "# Assuming profit marging to be of 5%\n",
        "customer_level_data[\"profit\"] = customer_level_data[\"spent_money\"]* 0.05\n",
        "customer_level_data.head()\n",
        "\n",
        "### CLV\n",
        "\n",
        "# CLV\n",
        "customer_level_data['CLV']=(customer_level_data['avg_order_value']*purchase_frequency)/churn_rate\n",
        "\n",
        "### CLTV\n",
        "\n",
        "customer_level_data['cust_lifetime_value']=customer_level_data['CLV']*customer_level_data['profit']\n",
        "customer_level_data.head()\n",
        "\n",
        "cdnow = CDNOW_DATA.copy()\n",
        "\n",
        "cdnow['month_yr'] = cdnow['TransactionDate'].apply(lambda x: x.strftime('%b-%Y'))\n",
        "cdnow.head()\n",
        "\n",
        "sale=cdnow.pivot_table(index=['CustomerID'],columns=['month_yr'],values='Price',aggfunc='sum',fill_value=0).reset_index()\n",
        "sale.head()\n",
        "\n",
        "sale['CLV']=sale.iloc[:,2:].sum(axis=1)\n",
        "sale.head()\n",
        "\n",
        "cdnow[\"TransactionDate\"].max()\n",
        "\n",
        "# CLV prediction based on the latest 6 month sales data:\n",
        "\n",
        ">Predictive modeling on the customer lifetime value instead of the sales expected in next 90 days\n",
        "\n",
        "# Selecting features as the latest 6 month data\n",
        "\n",
        "X2=sale[['Jan-1998','Feb-1998', 'Mar-1998','Apr-1998','May-1998','Jun-1998']]\n",
        "y=sale[['CLV']]\n",
        "\n",
        "### XGBOOST\n",
        "\n",
        "xgb = XGBRegressor(objective='reg:squarederror', random_state = 123) #model\n",
        "\n",
        "# GridSearch with all the hyperparameters\n",
        "xgb =GridSearchCV(estimator=xgb, param_grid = dict(learning_rate = [0.01, 0.1, 0.3, 0.5]), scoring='neg_mean_absolute_error', refit= True, cv = 5)\n",
        "\n",
        "xgb.fit(X2, y) # Fitting the data\n",
        "\n",
        "xgb.best_score_ # NAE for the best model\n",
        "\n",
        "xgb.best_params_ # Hyper parameters for the best model\n",
        "\n",
        "predictions_xgb_reg_6model = xgb.predict(X2) # prediction\n",
        "\n",
        "XGBOOST_6M_REG = pd.DataFrame({\"prediction_xgb_6m\" : predictions_xgb_reg_6model.tolist()}).merge(X2,left_index=True, right_index=True).merge(sale[\"CustomerID\"],left_index=True, right_index=True)\n",
        "\n",
        "XGBOOST_6M_REG.to_csv(\"XGB_reg_6m_prediction.csv\") # Prediction to csv.\n",
        "\n",
        "### Linear Regression\n",
        "\n",
        "lr = LinearRegression()\n",
        "\n",
        "parameters = {'fit_intercept':[True,False],  'copy_X':[True, False]} # Hyperparameters\n",
        "lr_reg_model = GridSearchCV(lr,parameters, cv=5, scoring='neg_mean_absolute_error')\n",
        "lr_reg_model.fit(X2, y) #fittig the data\n",
        "\n",
        "lr_reg_model.best_score_ # NAE for the best model\n",
        "\n",
        "lr_reg_model.best_params_ # Hyperparameters for the best model.\n",
        "\n",
        "predictions_lr_reg_6model = lr_reg_model.predict(X2).reshape(-1)\n",
        "predictions_lr_reg_6model\n",
        "\n",
        "# Storing the prediction in a dataframe.\n",
        "LinearReg_6M_REG = pd.DataFrame({\"prediction_lr_6m\" : predictions_lr_reg_6model.tolist()}).merge(X2,left_index=True, right_index=True).merge(sale[\"CustomerID\"],left_index=True, right_index=True)\n",
        "\n",
        "LinearReg_6M_REG.to_csv(\"lr_reg_6m_prediction.csv\")# Prediction to csv.\n",
        "\n",
        "### Random Forest Regression\n",
        "\n",
        "rf = RandomForestRegressor() # Model\n",
        "rf_regressor =GridSearchCV(estimator=rf, param_grid = param_grid, scoring='neg_mean_absolute_error', refit= True, cv = 5) \n",
        "\n",
        "rf_regressor.fit(X2, y.values.ravel()) # fitting the data\n",
        "\n",
        "rf_regressor.best_score_ # NAE for the best model\n",
        "\n",
        "predictions_rf_reg_6model = rf_regressor.predict(X2) # Prediction\n",
        "\n",
        "RFReg_6M_REG = pd.DataFrame({\"prediction_rf_6m\" : predictions_rf_reg_6model.tolist()}).merge(X2,left_index=True, right_index=True).merge(sale[\"CustomerID\"],left_index=True, right_index=True)\n",
        "\n",
        "RFReg_6M_REG.to_csv(\"rf_reg_6m_prediction.csv\")# Prediction to csv.\n",
        "\n",
        "# Lifetimes Library\n",
        "\n",
        "CDNOW_DATA = CDNOW_DATA[CDNOW_DATA['Price'] > 0]\n",
        "TRAIN = CDNOW_DATA[CDNOW_DATA[\"TransactionDate\"] < CDNOW_DATA[\"TransactionDate\"].max() - dt.timedelta(days = 30)]\n",
        "TRAIN.shape\n",
        "\n",
        "TEST =  CDNOW_DATA[CDNOW_DATA[\"TransactionDate\"] >= CDNOW_DATA[\"TransactionDate\"].max() - dt.timedelta(days = 30)]\n",
        "TEST.shape\n",
        "\n",
        "CDNOW_data_LT = lifetimes.utils.summary_data_from_transaction_data(TRAIN, 'CustomerID', 'TransactionDate', 'Price' )\n",
        "CDNOW_data_LT.head()\n",
        "# frequency: total number of repeat purchases\n",
        "# recency: the difference between the customer's last purchase and his first purchase\n",
        "# T: the age of the client in the company\n",
        "# monetary_value: average earnings per purchase\n",
        "\n",
        "# Create a distribution of frequency to understand the customer frequency level\n",
        "CDNOW_data_LT['frequency'].plot(kind='hist', bins=50)\n",
        "plt.show()\n",
        "print(CDNOW_data_LT['frequency'].describe())\n",
        "print(\"---------------------------------------\")\n",
        "one_time_buyers = round(sum(CDNOW_data_LT['frequency'] == 0)/float(len(CDNOW_data_LT))*(100),2)\n",
        "print(\"Percentage of customers that purchased the item only once:\", one_time_buyers ,\"%\")\n",
        "\n",
        "# Fitting the BG/NBD model\n",
        "bgf = lifetimes.BetaGeoFitter(penalizer_coef=0.0)\n",
        "bgf.fit(CDNOW_data_LT['frequency'], CDNOW_data_LT['recency'], CDNOW_data_LT['T'])\n",
        "\n",
        "# Model summary\n",
        "bgf.summary\n",
        "\n",
        "# Compute the customer alive probability\n",
        "CDNOW_data_LT['probability_alive'] = bgf.conditional_probability_alive(CDNOW_data_LT['frequency'], CDNOW_data_LT['recency'], CDNOW_data_LT['T'])\n",
        "CDNOW_data_LT.head(10)\n",
        "\n",
        "# Visual representation of relationship between recency and frequency\n",
        "fig = plt.figure(figsize=(12,8))\n",
        "plot_probability_alive_matrix(bgf)\n",
        "plt.show()\n",
        "\n",
        "#Predict future transaction for the next 30 days based on historical dataa\n",
        "t = 30\n",
        "CDNOW_data_LT['pred_num_txn'] = round(bgf.conditional_expected_number_of_purchases_up_to_time(t, CDNOW_data_LT['frequency'], CDNOW_data_LT['recency'], CDNOW_data_LT['T']),2)\n",
        "CDNOW_data_LT.sort_values(by='pred_num_txn', ascending=False).head(10).reset_index()\n",
        "\n",
        "# Checking the relationship between frequency and monetary_value\n",
        "CDNOW_return_customers_summary = CDNOW_data_LT[CDNOW_data_LT['frequency']>0]\n",
        "print(CDNOW_return_customers_summary.shape)\n",
        "CDNOW_return_customers_summary.head()\n",
        "\n",
        "# Checking the relationship between frequency and monetary_value\n",
        "CDNOW_return_customers_summary[['frequency', 'monetary_value']].corr()\n",
        "\n",
        "# Modeling the monetary value using Gamma-Gamma Model\n",
        "ggf = lifetimes.GammaGammaFitter(penalizer_coef=0.01)\n",
        "ggf.fit(CDNOW_return_customers_summary['frequency'],\n",
        "       CDNOW_return_customers_summary['monetary_value'])\n",
        "\n",
        "# Summary of the fitted parameters\n",
        "ggf.summary\n",
        "\n",
        "# Calculating the conditional expected average profit for each customer per transaction\n",
        "CDNOW_data_LT_vF = CDNOW_data_LT[CDNOW_data_LT['monetary_value'] >0]\n",
        "CDNOW_data_LT_vF['exp_avg_sales'] = ggf.conditional_expected_average_profit(CDNOW_data_LT['frequency'],\n",
        "                                       CDNOW_data_LT['monetary_value'])\n",
        "CDNOW_data_LT_vF.head()\n",
        "\n",
        "# Checking the expected average value and the actual average value in the data to make sure the values are good\n",
        "print(f\"Expected Average Sales: {CDNOW_data_LT_vF['exp_avg_sales'].mean()}\")\n",
        "print(f\"Actual Average Sales: {CDNOW_data_LT_vF['monetary_value'].mean()}\")\n",
        "\n",
        "# Predicting Customer Lifetime Value for the next 30 days\n",
        "CDNOW_data_LT_vF['predicted_clv'] =      ggf.customer_lifetime_value(bgf,\n",
        "                                                               CDNOW_data_LT_vF['frequency'],\n",
        "                                                               CDNOW_data_LT_vF['recency'],\n",
        "                                                               CDNOW_data_LT_vF['T'],\n",
        "                                                               CDNOW_data_LT_vF['monetary_value'],\n",
        "                                                               time=1,     # lifetime in months\n",
        "                                                               freq='D',   # frequency in which the data is present(T)      \n",
        "                                                               discount_rate=0.01) # discount rate\n",
        "CDNOW_data_LT_vF.head()\n",
        "\n",
        "# Manual CLV pridiction\n",
        "CDNOW_data_LT_vF['manual_predicted_clv'] = CDNOW_data_LT_vF['pred_num_txn'] * CDNOW_data_LT_vF['exp_avg_sales']\n",
        "CDNOW_data_LT_vF.head()\n",
        "\n",
        "### TEST\n",
        "\n",
        "TEST.head()\n",
        "\n",
        "ACTUAL = TEST.groupby(\"CustomerID\")[\"Price\"].sum().reset_index() # Calculating the Sum of Price\n",
        "\n",
        "ACTUAL.rename(columns ={\"Price\":\"actual_amount_spent_in_next_30_days\"}, inplace = True) # renaming the columns\n",
        "\n",
        "# Actual vs Predicted Dataframe\n",
        "Pred_vs_Actual = CDNOW_data_LT_vF.merge(ACTUAL, on = \"CustomerID\", how = \"left\")\n",
        "Pred_vs_Actual.fillna(0, inplace = True)\n",
        "\n",
        "Pred_vs_Actual.head() # looking at the Actual vs Predicted Dataframe\n",
        "\n",
        "# difference between actual and predicted\n",
        "Pred_vs_Actual[\"difference\"] = Pred_vs_Actual[\"manual_predicted_clv\"] - Pred_vs_Actual[\"actual_amount_spent_in_next_30_days\"]\n",
        "\n",
        "Pred_vs_Actual.head()\n",
        "\n",
        "Pred_vs_Actual[\"difference\"].hist()\n",
        "\n",
        "Pred_vs_Actual[\"difference\"].describe()\n",
        "\n",
        "Pred_vs_Actual.sort_values('actual_amount_spent_in_next_30_days', ascending = False)\n",
        "\n",
        "# MAE of the Lifetimes.\n",
        "mean_absolute_error(Pred_vs_Actual[\"actual_amount_spent_in_next_30_days\"], Pred_vs_Actual[\"manual_predicted_clv\"])"
      ]
    }
  ]
}