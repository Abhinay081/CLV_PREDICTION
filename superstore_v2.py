# -*- coding: utf-8 -*-
"""Superstore_V2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OkEJKXB3kbl8Iy8YqRcu_3AAK2m0Z4vF

# Global SuperStore Data



*   The Global Super Store data set is a customer centric dataset, which has the data of all the orders that have been place through different vendors and markets.​

*   The dataset set has around 50000 rows and 24 columns.​

## Importing the necessary libraries
"""

import warnings
warnings.filterwarnings('ignore')
import pandas as pd

import numpy as np
import matplotlib.pyplot as plt
import datetime as dt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import RobustScaler
from sklearn.cluster import KMeans

# machine learning
from xgboost import XGBClassifier, XGBRegressor
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import mean_absolute_error

"""## Loading the SuperStore data"""

# Laoding the data into a dataframe.
super_store_data = pd.read_excel("/content/Global_superstore_2016.xlsx")

super_store_data.shape

super_store_data.info() 
# info() gives us the number of non null values and the data type of each column.

"""Dropping the Customer Name column, as it contains personally identifiable information."""

super_store_data = super_store_data.drop("Customer Name",axis=1)
# Looking at the top 5 rows of the dataframe.
super_store_data.head()

"""## Exploratory Data Analysis

Verifying if the data contains null values:
"""

super_store_data.isnull().sum()

"""As postal code doesn't play a key role for CLV analysis, we are ignoring the missing postal codes.

Performing statistical analysis on the applicable features:
"""

# looking at the min, max, mean, standard deviation and quartile values of the numerical values.
super_store_data[["Sales", "Quantity", "Discount", "Profit", "Shipping Cost"]].describe()

# First Transaction Date
super_store_data["Order Date"].min()

# Last Transaction Date
super_store_data["Order Date"].max()

# NUmber of Unique Customers
super_store_data["Customer ID"].nunique()

# Total Revenue.
super_store_data["Sales"].sum()

#Mean Sales value per transaction
super_store_data["Sales"].mean()

super_store_data["Sales"].median()

order_count = pd.DataFrame((super_store_data.groupby("Customer ID")["Order Date"].count()).sort_values(ascending=False))

order_count[order_count["Order Date"]>1].shape

#Average of mean sales per customer
super_store_data.groupby("Customer ID")["Sales"].mean().mean()

"""Analysing the Profit feature, as above noticed transactions with negative profit"""

super_store_data[super_store_data["Profit"]<0].transpose()

"""Observed that there are 12544 transations with negative profit. As not every transaction is profitable, considering the negative profit transactions as correct data"""

print("Count of orders per customer:")
pd.DataFrame(super_store_data["Customer ID"].value_counts())

print("Total unique customers:")
super_store_data["Customer ID"].nunique()

"""### How many orders are placed by most customers?"""

super_store_data["Customer ID"].value_counts()

plt.figure(figsize=(8,6))
sns.countplot(x=super_store_data["Customer ID"].value_counts().values)
plt.title("Customers order count")
plt.show()

"""From the above visual, I conclude that majority of the customers placed 1- 2 orders during the period of study

### How has the order count been over the time period?
"""

plt.figure(figsize=(8,6))
plt.hist(super_store_data["Order Date"])
plt.title("Order count during the period  2012-2015")
plt.xticks(rotation=45)
plt.show()

"""Noticed that the orders count increased as the time progressed

### What's the customer segment that's placing most orders?
"""

plt.figure(figsize=(8,6))
sns.countplot(data=super_store_data, x= "Segment")
plt.title("Customer segmentation")
plt.show()

"""Consumers segment is placing the most orders followed by corporate"""

super_store_data["City"].value_counts()

print("Orders have been shipped to {} cities".format(super_store_data["City"].nunique()))

super_store_data["Country"].value_counts()

print("Orders have been shipped to {} countries".format(super_store_data["Country"].nunique()))

"""### What's the customer base in each region?"""

super_store_data["Region"].value_counts()

plt.figure(figsize=(12,6))
sns.countplot(data=super_store_data, x= "Region")
plt.title("Region wise orders")
plt.xticks(rotation=90)
plt.show()

"""### What's the orders share from each region?"""

super_store_data["Market"].value_counts()

plt.figure(figsize=(12,6))
sns.countplot(data=super_store_data, x= "Market")
plt.title("Market wise orders")
plt.xticks(rotation=90)
plt.show()

"""### What category of products are most sold?"""

super_store_data["Category"].value_counts()

plt.figure(figsize=(8,6))
sns.countplot(data=super_store_data,x="Category")
plt.title("Category segmentation of orders")
plt.show()

"""From the above visual, it is clear that the Global superstore dataset consists of maily office supplies related orders during the period of study"""

super_store_data["Sub-Category"].value_counts()

plt.figure(figsize=(12,6))
sns.countplot(data=super_store_data, x="Sub-Category")
plt.xticks(rotation=90)
plt.title("Orders per sub category")
plt.show()

"""### Customers generating the most sales?"""

super_store_data.groupby("Customer ID")["Sales"].sum().sort_values(ascending=False)

plt.figure(figsize=(8,6))
plt.hist(super_store_data.groupby("Customer ID")["Sales"].sum().values)
plt.title("Total sale distribution per customer")
plt.show()

"""### Customers placing the most orders?"""

super_store_data.groupby("Customer ID")["Order ID"].count().sort_values(ascending=False)

"""## Cohort Analysis"""

# Customers first transaction. Considering the first transaction of the customers as a subset to perfrom our analysis.
GD_FIRST_TRANSACTION = super_store_data.sort_values(["Customer ID", "Order Date"]) \
                               .groupby("Customer ID").first()
GD_FIRST_TRANSACTION.shape

# first new customer.
GD_FIRST_TRANSACTION["Order Date"].min()

# last new customer
GD_FIRST_TRANSACTION["Order Date"].max()

# GETTING THE UNIQUE CUSTOMERS ID's
IDs = super_store_data['Customer ID'].unique()
IDs_SAMPLE = IDs[0:10]

# Creating a subset of the dataset with ten unique customer IDs
GD_SUBSET = super_store_data[super_store_data['Customer ID'].isin(IDs_SAMPLE)].groupby(['Customer ID', 'Order Date']).sum().reset_index()
GD_SUBSET.head()

import plotnine as pn
import plydata.cat_tools as cat

# Frequency plot of the ten unique customers, with the price and date of each transaction.
pn.ggplot(pn.aes('Order Date', 'Sales', group = 'Customer ID'), data = GD_SUBSET) + pn.geom_line() +pn.geom_point() +pn.facet_wrap('Customer ID') +pn.scale_x_date(date_breaks = '1 year', date_lables = '%Y')

"""## Recency-Frequency-Monetary (RFM) analysis:

For Recency, calculating the number of days between present date and date of last purchase for each customer.

For Frequency, calculating the number of orders for each customer.

For Monetary, calculating the sum of purchase price for each customer.
"""

transaction_data = super_store_data[['Customer ID','Order ID','Order Date','Sales', 'Quantity','Discount']].copy()
transaction_data

max_date_considered = transaction_data['Order Date'].max().date()
print("Date considered for calculating the recency: ", max_date_considered.strftime("%m-%d-%Y"))

# Count of days since their last purchase until current date
rfm = pd.DataFrame((max_date_considered - transaction_data.groupby("Customer ID")["Order Date"].max().apply(lambda x: x.date())).dt.days)
rfm

rfm.columns = ["Recency"]
rfm.head()

# Count of days between their last purchase and first purchase
transaction_data.groupby("Customer ID")["Order Date"].max().apply(lambda x: x.date()) - transaction_data.groupby("Customer ID")["Order Date"].min().apply(lambda x: x.date())

# Orders per customer
frequency_orders = transaction_data.groupby("Customer ID")["Order ID"].count()
frequency_orders

rfm = rfm.merge(frequency_orders.reset_index(), left_on = "Customer ID", right_on= "Customer ID")
rfm.head()

rfm.columns = ['Customer ID', 'Recency', 'Frequency']
rfm.head()



# Sales per customer
sales_per_customer = transaction_data.groupby("Customer ID")["Sales"].sum()
sales_per_customer.reset_index()

rfm = rfm.merge(sales_per_customer.reset_index(),left_on = "Customer ID", right_on="Customer ID")
rfm.head()

# Renaming the column names

rfm = rfm.set_index("Customer ID") #Setting customer ID as the index for rfm dataframe
rfm.columns = ["Recency", "Frequency", "Monetary"]
rfm.head()



sns.histplot(rfm["Recency"])
plt.title("Recency distribution of superstore data")
plt.show()

sns.histplot(rfm["Frequency"])
plt.title("Frequency distribution of superstore data")
plt.show()

sns.histplot(rfm[rfm["Frequency"]<15]["Frequency"])
plt.title("Frequency distribution(<15) of superstore data")
plt.show()

sns.histplot(rfm["Monetary"])
plt.title("Monetary distribution of superstore data")
plt.show()

sns.histplot(rfm[rfm["Monetary"]<5000]["Monetary"])
plt.title("Monetary distribution (< 5000) of superstore data")
plt.show()

"""### Computing Quantile of RFM values
Customers with the lowest recency, highest frequency and monetary amounts considered as top customers.
"""

rfm["Recency_quatile"] = pd.qcut(rfm["Recency"].rank(method="first"),4,['1','2','3','4']) 
# Using rank method to handle bins with duplicate edges
rfm["Frequency_quatile"] = pd.qcut(rfm["Frequency"].rank(method="first"), 4, ['4','3','2','1'])
rfm["Monetary_quantile"] = pd.qcut(rfm["Monetary"].rank(method="first"), 4, ['4','3','2','1'])
rfm.head()

"""RFM Result Calculation:

Combining all three quartiles(r_quartile,f_quartile,m_quartile) to a single column, to segment the customers
"""

rfm["RFM score"] = rfm[['Recency_quatile', 'Frequency_quatile', 'Monetary_quantile']].\
                    apply(lambda x: str(x[0])+str(x[1])+str(x[2]),axis=1)

rfm.head()

# Identifing the top customers

rfm[rfm["RFM score"] == '111'].sort_values("Monetary",ascending=False)

"""The top customer based on RFM analysis is 'SM-203201408'"""



"""### Using unsupervised learning - clustering based on each of RFM values

"""

rfm_clustering = rfm[['Recency','Frequency', 'Monetary']].copy()
rfm_clustering.head()

rfm_clustering = rfm_clustering.reset_index()
rfm_clustering

"""In order to perform k means clustering, scaling the data to avoid influence of scale during cluster identification"""



sns.boxplot(x = rfm_clustering["Recency"])
plt.title("Box plot of Recency")
plt.show()

sns.boxplot(x = rfm_clustering["Frequency"])
plt.title("Box plot of Frequency")
plt.show()

rfm_clustering[rfm_clustering["Frequency"]>7].shape

sns.boxplot(x = rfm_clustering["Monetary"])
plt.title("Box plot of Monetary value")
plt.show()

rfm_clustering[rfm_clustering["Monetary"]>2500].shape

"""As the box plot indicates, outliers in Frequency and Monetary values, of all the available scaling techniques, using the RobustScalar as the scaling happens based on percentiles and is not influenced by the few marginal outleirs in the data."""

from sklearn.preprocessing import RobustScaler

rscaler = RobustScaler()

rfm_clustering["Recency_norm"] = rscaler.fit_transform(rfm_clustering[["Recency"]])

rfm_clustering["Frequency_norm"] = rscaler.fit_transform(rfm_clustering[["Frequency"]])
rfm_clustering["Monetary_norm"] = rscaler.fit_transform(rfm_clustering[["Monetary"]])

rfm_clustering.head()

rfm_clustering[['Recency_norm', 'Frequency_norm', 'Monetary_norm']].describe()

from sklearn.cluster import KMeans

sse={}
recency = rfm_clustering[['Recency_norm']].copy()
for k in range(1, 10):
    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(recency)
    recency["clusters"] = kmeans.labels_
    sse[k] = kmeans.inertia_ 
plt.figure()
plt.plot(list(sse.keys()), list(sse.values()))
plt.title("Elbow plot of recency KMeans clustering")
plt.xlabel("Number of cluster")
plt.show()

kmeans = KMeans(n_clusters=4)
kmeans.fit(recency[['Recency_norm']])
rfm_clustering['RecencyCluster'] = kmeans.predict(rfm_clustering[['Recency_norm']])

rfm_clustering

def order_cluster(cluster_field, target_field,df,ascending):
    """
    function to assign the label values based on a particular sorting order defined as a parameter
    
    cluster_field: Field/column name storing the cluster
    target_field: Field storing the respective rfm actual values
    df: dataframe
    ascending: True/False
    
    """
    
    df_new = df.groupby(cluster_field)[target_field].mean().reset_index()
    df_new = df_new.sort_values(by=target_field,ascending=ascending).reset_index(drop=True)
    df_new['index'] = df_new.index
    df_final = pd.merge(df,df_new[[cluster_field,'index']], on=cluster_field)
    df_final = df_final.drop([cluster_field],axis=1)
    df_final = df_final.rename(columns={"index":cluster_field})
    return df_final

rfm_clustering = order_cluster("RecencyCluster", "Recency", rfm_clustering,False)
rfm_clustering

rfm_clustering["RecencyCluster"].value_counts()

sse={}
frequency = rfm_clustering[['Frequency_norm']].copy()
for k in range(1, 10):
    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(frequency)
    frequency["clusters"] = kmeans.labels_
    sse[k] = kmeans.inertia_ 
plt.figure()
plt.plot(list(sse.keys()), list(sse.values()))
plt.title("Elbow plot of frequency KMeans clustering")
plt.xlabel("Number of cluster")
plt.show()

kmeans = KMeans(n_clusters=4)
kmeans.fit(rfm_clustering[['Frequency_norm']])
rfm_clustering['FrequencyCluster'] = kmeans.predict(rfm_clustering[['Frequency_norm']])

rfm_clustering = order_cluster("FrequencyCluster", "Frequency", rfm_clustering,True)
rfm_clustering



sse={}
monetary = rfm_clustering[['Monetary_norm']].copy()
for k in range(1, 10):
    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(monetary)
    monetary["clusters"] = kmeans.labels_
    sse[k] = kmeans.inertia_ 
plt.figure()
plt.plot(list(sse.keys()), list(sse.values()))
plt.title("Elbow plot of frequency KMeans clustering")
plt.xlabel("Number of cluster")
plt.show()

kmeans = KMeans(n_clusters=3)
kmeans.fit(rfm_clustering[['Monetary_norm']])
rfm_clustering['MonetaryCluster'] = kmeans.predict(rfm_clustering[['Monetary']])

rfm_clustering = order_cluster("MonetaryCluster", "Monetary", rfm_clustering,True)
rfm_clustering

rfm_clustering['OverallScore'] = rfm_clustering['RecencyCluster'] + rfm_clustering['FrequencyCluster'] + rfm_clustering['MonetaryCluster']

rfm_clustering["OverallScore"].value_counts()

rfm_clustering.groupby(["OverallScore"])[["Recency","Frequency", "Monetary"]].mean()

rfm_clustering.groupby('OverallScore')['Customer ID'].count()

rfm_clustering



rfm_clustering['Segment'] = 'Lost Customer'
rfm_clustering.loc[rfm_clustering['OverallScore']>2,'Segment'] = 'Low-Value' 
rfm_clustering.loc[rfm_clustering['OverallScore']>3,'Segment'] = 'Mid-Value'
rfm_clustering.loc[rfm_clustering['OverallScore']>5,'Segment'] = 'High-Value'
rfm_clustering.loc[rfm_clustering['OverallScore']>7,'Segment'] = 'Top Customer'

sns.countplot(x=rfm_clustering["Segment"],order= ['Lost Customer','Low-Value' ,'Mid-Value','High-Value','Top Customer'])
plt.title("RFM segmentation of super-store data")
plt.show()

"""Here, Top customers are the ones with low recency, high frequency and high monetary values; whereas lost customers are the ones who have high recency, low frequency and low monetary value"""

rfm_clustering['Segment'].value_counts()

rfm_clustering[rfm_clustering["Segment"]=='Top Customer']

rfm_clustering[["Customer ID", 'Monetary', 'Frequency','Recency', 'RecencyCluster', 'FrequencyCluster',
       'MonetaryCluster', 'OverallScore', 'Segment']].to_csv("RFM_segmentation.csv")

super_store_data.head()

super_store_data.columns

"""## Machine Learning Modelling
### Time Split

Performing time split, to split the data into train and test wherein test data, indicates if the customer purchased in the following 90 days and if yes, what's the sale amount
"""

n_days = 90   
max_date = super_store_data['Order Date'].max()
cutoff = max_date - pd.to_timedelta(n_days, unit = 'd')

temporal_in_df = super_store_data[super_store_data['Order Date']<= cutoff][['Order Date', 
       'Customer ID', 'Sales']]
temporal_out_df = super_store_data[super_store_data['Order Date']> cutoff][['Order Date', 
       'Customer ID', 'Sales']]

"""## Feature Engineering"""

#make targets from out data
targets_df = temporal_out_df.groupby('Customer ID').sum().rename({'Sales':'spend_90_total'}, axis = 1).assign(spend_90_flag = 1)
targets_df

targets_df["spend_90_total"].sort_values()

# make recency (date) features from in data
max_date = temporal_in_df['Order Date'].max()

recency_features_df = temporal_in_df[['Customer ID', 'Order Date']].groupby('Customer ID').apply(lambda x: (x['Order Date'].max() - max_date)/ pd.to_timedelta(1, 'day')).to_frame().set_axis(['recency'], axis = 1)

recency_features_df.head()

# make frequency (count features from in data)
frequency_features_df = temporal_in_df[['Customer ID', 'Order Date']].groupby('Customer ID').count().set_axis(['frequency'], axis = 1)
frequency_features_df.head()

# make price (monitery features from in data)
price_features_df = temporal_in_df.groupby('Customer ID').aggregate({'Sales':['sum', 'mean']}).set_axis(['price_sum', 'price_mean'], axis = 1)
price_features_df.head()

# combine features
features_df = pd.concat([recency_features_df, frequency_features_df, price_features_df], axis = 1).merge(targets_df, left_index = True, right_index = True, how = 'left').fillna(0)
features_df

"""## Perfroming various predictive modelling techniques:

### Predictve modelling for next 90 days behaviour:

#### How much will the customers spend in the next 90 days?
"""

# machine laerning

from xgboost import XGBClassifier, XGBRegressor
from sklearn.model_selection import cross_val_score, GridSearchCV

features_df.loc['AA-10315139',:]

X = features_df[['recency', 'frequency', 'price_sum', 'price_mean']]

# next 90 days spend prediction
y_spend = features_df['spend_90_total']

X.shape

y_spend.shape

"""##### XGBoost"""

xgb_reg_spec = XGBRegressor(objective='reg:squarederror', random_state=123)

# Creating a GridSearchCV to tune the hyperparameters
xgb_reg_model =GridSearchCV(estimator=xgb_reg_spec, param_grid = dict(learning_rate = [0.01, 0.1, 0.3, 0.5]), scoring='neg_mean_absolute_error', refit= True, cv = 5)

# Fitting the Model
xgb_reg_model.fit(X, y_spend)

xgb_reg_model.best_score_ # NAME of the best parameters model.

xgb_reg_model.best_params_ # best Hyperparameter value

xgb_reg_model.best_estimator_

# Merging the predictions to the data.
predictions_xgb_reg_model = xgb_reg_model.predict(X)

predictions_xgb_reg_model

pd.DataFrame({"prediction_xgb" : predictions_xgb_reg_model.tolist()}).merge(X.reset_index(),left_index=True, right_index=True)

pd.DataFrame({"prediction_xgb" : predictions_xgb_reg_model.tolist()}).merge(X.reset_index(),left_index=True, right_index=True).to_csv("XGB_reg_prediction.csv")

"""##### Linear Regression"""

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler

lr = LinearRegression()

parameters = {'fit_intercept':[True,False],  'copy_X':[True, False]}
lr_reg_model = GridSearchCV(lr,parameters, cv=5, scoring='neg_mean_absolute_error')
lr_reg_model.fit(X, y_spend)

lr_reg_model.best_score_

lr_reg_model.best_params_

predictions_lr_reg_model = lr_reg_model.predict(X)

pd.DataFrame({"prediction_lr" : predictions_lr_reg_model.tolist()}).merge(X.reset_index(),left_index=True, right_index=True)

pd.DataFrame({"prediction_lr" : predictions_lr_reg_model.tolist()}).merge(X.reset_index(),left_index=True, right_index=True).to_csv("lr_reg_prediction.csv")

"""##### Random Forest Regressor"""

from sklearn.ensemble import RandomForestRegressor

param_grid = dict(n_estimators = [10, 20, 50, 100, 500, 1000], max_depth = [2, 5], max_features = ['auto', 'sqrt', 'log2'])



rf = RandomForestRegressor()
rf_regressor =GridSearchCV(estimator=rf, param_grid = param_grid, scoring='neg_mean_absolute_error', refit= True, cv = 5)

rf_regressor.fit(X, y_spend)

rf_regressor.best_score_

predictions_rf_reg_model = rf_regressor.predict(X)

pd.DataFrame({"prediction_rf" : predictions_rf_reg_model.tolist()}).merge(X.reset_index(),left_index=True, right_index=True)

pd.DataFrame({"prediction_rf" : predictions_rf_reg_model.tolist()}).merge(X.reset_index(),left_index=True, right_index=True).to_csv("rf_reg_prediction.csv")

"""#### Will a customer make a purchase in next 90 days"""

# Next 90 days spend probability
y_prob = features_df['spend_90_flag']

xgb_clf_spec = XGBClassifier(objective='binary:logistic', random_state= 123)

"""##### XGBoost"""

xgb_clf_model =GridSearchCV(estimator=xgb_clf_spec, param_grid = dict(learning_rate = [0.01, 0.1, 0.3, 0.5]), scoring='roc_auc', refit= True, cv = 5)

xgb_clf_model.fit(X, y_prob)

xgb_clf_model.best_score_

xgb_clf_model.best_params_

xgb_clf_model.best_estimator_

predictions_xgb_clf_model = xgb_clf_model.predict(X)

pd.DataFrame(xgb_clf_model.predict(X)).value_counts()

#pd.DataFrame({"prediction_xgb_clf" : predictions_xgb_clf_model.tolist()}).merge(X.reset_index(),left_index=True, right_index=True)
pd.DataFrame(xgb_clf_model.predict_proba(X),columns=['prediction_xgb_clf_0','prediction_xgb_clf_1'])

pd.DataFrame(xgb_clf_model.predict_proba(X),columns=['prediction_xgb_clf_0','prediction_xgb_clf_1']).merge(X.reset_index(),left_index=True, right_index=True)

pd.DataFrame(xgb_clf_model.predict_proba(X),columns=['prediction_xgb_clf_0','prediction_xgb_clf_1']).merge(X.reset_index(),left_index=True, right_index=True).to_csv("xgb_clf_prediction.csv")

"""##### Logistic regression"""

from sklearn.linear_model import LogisticRegression

parameters = {'penalty': ["l1", "l2"], 'solver': ["liblinear","saga"]}

logr = LogisticRegression()
logr_clf_model = GridSearchCV(logr,parameters, cv=5, scoring='roc_auc')
logr_clf_model.fit(X, y_prob)

logr_clf_model.best_params_

logr_clf_model.best_score_

pd.DataFrame(logr_clf_model.predict_proba(X),columns=['prediction_log_clf_0','prediction_log_clf_1']).merge(X.reset_index(),left_index=True, right_index=True)

pd.DataFrame(logr_clf_model.predict_proba(X),columns=['prediction_log_clf_0','prediction_log_clf_1']).merge(X.reset_index(),left_index=True, right_index=True).to_csv("log_clf_prediction.csv")

"""##### Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier

parameters = { 'max_depth' : [2, 5], 'n_estimators' : [10, 20, 50, 100, 500, 1000],'max_features' : ['auto', 'sqrt', 'log2']}

rfc = RandomForestClassifier()
rfc_clf_model = GridSearchCV(rfc,parameters, cv=5, scoring='roc_auc')
rfc_clf_model.fit(X, y_prob)

rfc_clf_model.best_params_

rfc_clf_model.best_score_

pd.DataFrame(rfc_clf_model.predict_proba(X),columns=['prediction_rf_clf_0','prediction_rf_clf_1']).merge(X.reset_index(),left_index=True, right_index=True)

pd.DataFrame(rfc_clf_model.predict_proba(X),columns=['prediction_rf_clf_0','prediction_rf_clf_1']).merge(X.reset_index(),left_index=True, right_index=True).to_csv("rf_clf_prediction.csv")



!pip install plydata

import joblib

import plydata.cat_tools as cat

"""As XGBoostRegressor performed relatively better, considering it for further analysis:

#### Feature Importance
"""

# Feature importance (global)
#importance | spend amount model

imp_spend_amount_dict = xgb_reg_model.best_estimator_.get_booster().get_score(importance_type = 'gain')

imp_spend_amount_df = pd.DataFrame(data = {'feature': list(imp_spend_amount_dict.keys()), 
                                   'value': list(imp_spend_amount_dict.values())}).assign(feature = lambda x : cat.cat_reorder(x['feature'], x['value']))

imp_spend_amount_df

# Importance | Spend probability model
imp_spend_prob_dict = xgb_clf_model.best_estimator_.get_booster().get_score(importance_type = 'gain')

imp_spend_prob_df = pd.DataFrame(data = {'feature': list(imp_spend_prob_dict.keys()), 
                                   'value': list(imp_spend_prob_dict.values())}).assign(feature = lambda x : cat.cat_reorder(x['feature'], x['value']))

imp_spend_prob_df

# save predictions
#predictions_df = pd.concat([pd.DataFrame(predictions_reg).set_axis(['pred_spend'], axis = 1), pd.DataFrame(predictions_clf)[[1]].set_axis(['pred_prob'], axis = 1), features_df.reset_index()], axis =1)

predictions_df = pd.DataFrame({"prediction_xgb" : predictions_xgb_reg_model.tolist()}).merge(X.reset_index(),left_index=True, right_index=True)
predictions_df.columns = ['spend_90_total', 'Customer ID', 'recency', 'frequency', 'price_sum','price_mean']
predictions_df

predictions_df = predictions_df.merge((pd.DataFrame(xgb_clf_model.predict_proba(X),columns=['prediction_xgb_clf_0','prediction_xgb_clf_1']).merge(X.reset_index()["Customer ID"],left_index=True, right_index=True)), on="Customer ID")
predictions_df

predictions_df[predictions_df["spend_90_total"]>0]["Customer ID"].count()

## Which customers have the highest spend probability in the next 90 days
  ##Target for new products similar to what they have purchased in the past
predictions_df.sort_values('prediction_xgb_clf_1', ascending = False)

"""##Which customers have recently purchased but are unlikely to buy
- Provide discounts, incentives to increase probability
- encourage referring a friend
"""

predictions_df[predictions_df['recency']>-90][predictions_df['prediction_xgb_clf_1']<0.3].sort_values('prediction_xgb_clf_1', ascending = False)

## Missed opportunities- Big spenders that could be unlocked
  ## send bundle offers encouraging volume purchases
  ## focus on missed opportunities
predictions_df[predictions_df['spend_90_total'] == 0.0].sort_values('prediction_xgb_clf_1', ascending = False)



"""## CLV prediction based on the latest 6 month sales data:

Here, I am performing predictive modeling on the customer lifetime value instead of the sales expected in next 90 days

CLTV = ((Average Order Value x Purchase Frequency)/Churn Rate) x Profit margin.

 Customer Value = Average Order Value * Purchase Frequency
"""

super_store_data

"""Profit margin is the commonly used profitability ratio. It represents how much percentage of total sales has earned as the gain."""

profit_margin = super_store_data["Profit"].sum()/ (super_store_data["Sales"].sum())

# Creating a customer level dataframe
customer_level_data = super_store_data.groupby("Customer ID").aggregate({"Order Date": lambda x: (x.max() - x.min()).days,
                                                   "Order ID": lambda x: len(x),
                                                   "Sales" : lambda x: x.sum()})
customer_level_data.columns = ['num_days','num_transactions','spent_money']
customer_level_data

#Calculating the average order value for each customer
customer_level_data['avg_order_value']=customer_level_data['spent_money']/customer_level_data['num_transactions']

customer_level_data.head()

purchase_frequency=sum(customer_level_data['num_transactions'])/customer_level_data.shape[0]
purchase_frequency

# Repeat Rate
repeat_rate=customer_level_data[customer_level_data.num_transactions > 1].shape[0]/customer_level_data.shape[0]
repeat_rate

#Churn Rate
churn_rate=1-repeat_rate
churn_rate

purchase_frequency,repeat_rate,churn_rate

customer_level_data["profit"] = customer_level_data["spent_money"]* profit_margin
customer_level_data.head()

customer_level_data['CLV']=(customer_level_data['avg_order_value']*purchase_frequency)/churn_rate

#Customer Lifetime Value
customer_level_data['cust_lifetime_value']=customer_level_data['CLV']*customer_level_data['profit']
customer_level_data.head()

super_store_data2 = super_store_data.copy()

super_store_data2['month_yr'] = super_store_data2['Order Date'].apply(lambda x: x.strftime('%b-%Y'))
super_store_data2.head()

sale=super_store_data2.pivot_table(index=['Customer ID'],columns=['month_yr'],values='Sales',aggfunc='sum',fill_value=0).reset_index()
sale.head()

sale['CLV']=sale.iloc[:,2:].sum(axis=1)
sale.head()

super_store_data2["Order Date"].max()

# Selecting features as the latest 6 month data

X2=sale[['Dec-2015','Nov-2015', 'Oct-2015','Sep-2015','Aug-2015','Jul-2015']]
y=sale[['CLV']]



"""#### Performing CLV predictive modelling:

##### XGBoost
"""

xgb = XGBRegressor(objective='reg:squarederror', random_state=123)

xgb =GridSearchCV(estimator=xgb, param_grid = dict(learning_rate = [0.01, 0.1, 0.3, 0.5]), scoring='neg_mean_absolute_error', refit= True, cv = 5)

xgb.fit(X2, y)

xgb.best_score_

xgb.best_params_

xgb.best_estimator_

X2.merge(sale["Customer ID"],left_index=True, right_index=True)

predictions_xgb_reg_6model = xgb.predict(X2)

len(predictions_xgb_reg_6model[predictions_xgb_reg_6model>0])

pd.DataFrame({"prediction_xgb_6m" : predictions_xgb_reg_6model.tolist()}).merge(X2,left_index=True, right_index=True).merge(sale["Customer ID"],left_index=True, right_index=True)

pd.DataFrame({"prediction_xgb_6m" : predictions_xgb_reg_6model.tolist()}).merge(X2,left_index=True, right_index=True).merge(sale["Customer ID"]
                                                                                                                            ,left_index=True, right_index=True)\
.to_csv("XGB_reg_6m_prediction.csv")

"""##### Linear regression"""

lr = LinearRegression()

parameters = {'fit_intercept':[True,False],  'copy_X':[True, False]}
lr_reg_model = GridSearchCV(lr,parameters, cv=5, scoring='neg_mean_absolute_error')
lr_reg_model.fit(X2, y)

lr_reg_model.best_score_

lr_reg_model.best_params_

predictions_lr_reg_6model = lr_reg_model.predict(X2).reshape(-1)
predictions_lr_reg_6model

pd.DataFrame({"prediction_lr_6m" : predictions_lr_reg_6model.tolist()}).merge(X2,left_index=True, right_index=True).merge(sale["Customer ID"],left_index=True, right_index=True)

len(predictions_lr_reg_6model[predictions_lr_reg_6model>0])

pd.DataFrame({"prediction_lr_6m" : predictions_lr_reg_6model.tolist()}).merge(X2,left_index=True, right_index=True)\
.merge(sale["Customer ID"],left_index=True, right_index=True)\
.to_csv("lr_reg_6m_prediction.csv")





"""##### Randomforest"""

rf = RandomForestRegressor()
rf_regressor =GridSearchCV(estimator=rf, param_grid = param_grid, scoring='neg_mean_absolute_error', refit= True, cv = 5)

rf_regressor.fit(X2, y.values.ravel())

rf_regressor.best_score_

predictions_rf_reg_6model = rf_regressor.predict(X2)

len(predictions_rf_reg_6model[predictions_rf_reg_6model>0])

pd.DataFrame({"prediction_rf_6m" : predictions_rf_reg_6model.tolist()}).merge(X2,left_index=True, right_index=True).merge(sale["Customer ID"],left_index=True, right_index=True)

pd.DataFrame({"prediction_rf_6m" : predictions_rf_reg_6model.tolist()}).merge(X2,left_index=True, right_index=True)\
.merge(sale["Customer ID"],left_index=True, right_index=True)\
.to_csv("rf_reg_6m_prediction.csv")



"""## CLV prediction via probabilistic methods - Lifetimes package:"""

pip install lifetimes

import lifetimes



# Creating the summary data using summary_data_from_transaction_data function
summary = lifetimes.utils.summary_data_from_transaction_data(super_store_data, 'Customer ID', 'Order Date', 'Sales' )
summary = summary.reset_index()
summary.head()

# Create a distribution of frequency to understand the customer frequence level
summary['frequency'].plot(kind='hist', bins=50)

one_time_buyers = round(sum(summary['frequency'] == 0)/float(len(summary))*(100),2)
print("Percentage of customers purchase the item only once:", one_time_buyers ,"%")

import datetime as dt
TRAIN = super_store_data[super_store_data["Order Date"] < super_store_data["Order Date"].max() - dt.timedelta(days = 30)]
TRAIN.shape

TEST =  super_store_data[super_store_data["Order Date"] >= super_store_data["Order Date"].max() - dt.timedelta(days = 30)]
TEST.shape

GD_data_LT = lifetimes.utils.summary_data_from_transaction_data(TRAIN, 'Customer ID', 'Order Date', 'Sales' )
GD_data_LT.head()

# frequency: total number of repeat purchases
# recency: the difference between the customer's last purchase and his first purchase
# T: the age of the client in the company
# monetary_value: average earnings per purchase

# Create a distribution of frequency to understand the customer frequency level
GD_data_LT['frequency'].plot(kind='hist', bins=50)
print(GD_data_LT['frequency'].describe())
print("---------------------------------------")
one_time_buyers = round(sum(GD_data_LT['frequency'] == 0)/float(len(GD_data_LT))*(100),2)
print("Percentage of customers that purchased the item only once:", one_time_buyers ,"%")

# Fitting the BG/NBD model
bgf = lifetimes.BetaGeoFitter(penalizer_coef=0.0)
bgf.fit(GD_data_LT['frequency'], GD_data_LT['recency'], GD_data_LT['T'])
# Model summary
bgf.summary

# Compute the customer alive probability
GD_data_LT['probability_alive'] = bgf.conditional_probability_alive(GD_data_LT['frequency'], GD_data_LT['recency'], GD_data_LT['T'])
GD_data_LT.head(10)

# Visual representation of relationship between recency and frequency
import matplotlib.pyplot as plt
from lifetimes.plotting import plot_probability_alive_matrix

fig = plt.figure(figsize=(12,8))
plot_probability_alive_matrix(bgf)
plt.show()

#Predict future transaction for the next 30 days based on historical dataa
t = 30
GD_data_LT['pred_num_txn'] = round(bgf.conditional_expected_number_of_purchases_up_to_time(t, GD_data_LT['frequency'], GD_data_LT['recency'], GD_data_LT['T']),2)
GD_data_LT.sort_values(by='pred_num_txn', ascending=False).head(10).reset_index()

# Checking the relationship between frequency and monetary_value
GD_return_customers_summary = GD_data_LT[GD_data_LT['frequency']>0]
print(GD_return_customers_summary.shape)
GD_return_customers_summary.head()

# Checking the relationship between frequency and monetary_value
GD_return_customers_summary[['frequency', 'monetary_value']].corr()

# Modeling the monetary value using Gamma-Gamma Model
ggf = lifetimes.GammaGammaFitter(penalizer_coef=0.01)
ggf.fit(GD_return_customers_summary['frequency'], GD_return_customers_summary['monetary_value'])
# Summary of the fitted parameters
ggf.summary

# Calculating the conditional expected average profit for each customer per transaction
GD_data_LT_vF = GD_data_LT[GD_data_LT['monetary_value'] >0]
GD_data_LT_vF['exp_avg_sales'] = ggf.conditional_expected_average_profit(GD_data_LT_vF['frequency'],
                                       GD_data_LT_vF['monetary_value'])
GD_data_LT_vF.head()

# Checking the expected average value and the actual average value in the data to make sure the values are good
print(f"Expected Average Sales: {GD_data_LT_vF['exp_avg_sales'].mean()}")
print(f"Actual Average Sales: {GD_data_LT_vF['monetary_value'].mean()}")

# Predicting Customer Lifetime Value for the next 30 days
GD_data_LT_vF['predicted_clv'] =      ggf.customer_lifetime_value(bgf,
                                                               GD_data_LT_vF['frequency'],
                                                               GD_data_LT_vF['recency'],
                                                               GD_data_LT_vF['T'],
                                                               GD_data_LT_vF['monetary_value'],
                                                               time=1,     # lifetime in months
                                                               freq='D',   # frequency in which the data is present(T)      
                                                               discount_rate=0.01) # discount rate
GD_data_LT_vF.head()

GD_data_LT_vF['manual_predicted_clv'] = GD_data_LT_vF['pred_num_txn'] * GD_data_LT_vF['exp_avg_sales']
GD_data_LT_vF.head()

"""#### Test"""

TEST.head()

ACTUAL = TEST.groupby("Customer ID")["Sales"].sum().reset_index()

ACTUAL.rename(columns ={"Sales":"actual_amount_spent_in_next_30_days"}, inplace = True)

Pred_vs_Actual = GD_data_LT_vF.merge(ACTUAL, on = "Customer ID", how = "left")
Pred_vs_Actual.fillna(0, inplace = True)

Pred_vs_Actual.head()

Pred_vs_Actual["difference"] = Pred_vs_Actual["manual_predicted_clv"] - Pred_vs_Actual["actual_amount_spent_in_next_30_days"]

Pred_vs_Actual.head()

Pred_vs_Actual["difference"].hist()

Pred_vs_Actual["difference"].describe()

Pred_vs_Actual.sort_values('actual_amount_spent_in_next_30_days', ascending = False)



# MAE of the Lifetimes based prediction
mean_absolute_error(Pred_vs_Actual["actual_amount_spent_in_next_30_days"],Pred_vs_Actual["manual_predicted_clv"])

Pred_vs_Actual.to_csv("lifetimes_based_global_datastore.csv")

